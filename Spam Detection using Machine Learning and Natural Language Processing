Appendices A.Source code 1. Module – Data Processing
import re
from nltk.tokenize import sent_tokenize,word_tokenize from nltk import pos_tag from nltk.corpus import wordnet as wn from nltk.corpus import stopwords from nltk.stem.wordnet import WordNetLemmatizer from collections import defaultdict import spacy
tag_map = defaultdict(lambda : wn.NOUN) tag_map['J'] = wn.ADJ tag_map['V'] = wn.VERB tag_map['R'] = wn.ADV lemmatizer=WordNetLemmatizer() stop_words=set(stopwords.words('english')) nlp=spacy.load('en_core_web_sm')
def process_sentence(sentence):
nouns = list() base_words = list() final_words = list() words_2 = word_tokenize(sentence) sentence = re.sub(r'[^ \w\s]', '', sentence) sentence = re.sub(r'_', ' ', sentence) words = word_tokenize(sentence) pos_tagged_words = pos_tag(words) for token, tag in pos_tagged_words:
base_words.append(lemmatizer.lemmatize(token,tag_map[tag[0]])) for word in base_words:
if word not in stop_words:
final_words.append(word)
sym = ' '
sent = sym.join(final_words) pos_tagged_sent = pos_tag(words_2) for token, tag in pos_tagged_sent:
if tag == 'NN' and len(token)>1:
nouns.append(token)
return sent, nouns
def clean(email):
email = email.lower() sentences = sent_tokenize(email) total_nouns = list() string = "" for sent in sentences:
sentence, nouns = process_sentence(sent)
string += " " + sentence total_nouns += nouns
return string, nouns
def ents(text): doc = nlp(text) expls = dict() if doc.ents:
for ent in doc.ents: labels = list(expls.keys()) label = ent.label_ word = ent.text if label in labels: words = expls[label] words.append(word) expls[label] = words
else: expls[label] = [word]
return expls
else:
return 'no'
2. Module – Machine Learning
from sklearn.feature_extraction.text import
CountVectorizer,TfidfVectorizer import numpy as np
from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier import pandas as pd
class model:
def __init__(self):
self.df = pd.read_csv('Cleaned_Data.csv') self.df['Email'] = self.df.Email.apply(lambda email:
np.str_(email)) self.Data = self.df.Email self.Labels = self.df.Label self.training_data, self.testing_data,
self.training_labels, self.testing_labels = train_test_split(self.Data,self.Labels,random_state=10) self.training_data_list = self.training_data.to_list() self.vectorizer = TfidfVectorizer() self.training_vectors =
self.vectorizer.fit_transform(self.training_data_list) self.model_nb = MultinomialNB() self.model_svm = SVC(probability=True) self.model_lr = LogisticRegression()
self.model_knn = KNeighborsClassifier(n_neighbors=9) self.model_rf = RandomForestClassifier(n_estimators=19) self.model_nb.fit(self.training_vectors,
self.training_labels) self.model_lr.fit(self.training_vectors,
self.training_labels) self.model_rf.fit(self.training_vectors,
self.training_labels) self.model_knn.fit(self.training_vectors,
self.training_labels) self.model_svm.fit(self.training_vectors,
self.training_labels) def get_prediction(self,vector):
pred_nb=self.model_nb.predict(vector)[0] pred_lr=self.model_lr.predict(vector)[0] pred_rf=self.model_rf.predict(vector)[0] pred_svm=self.model_svm.predict(vector)[0] pred_knn=self.model_knn.predict(vector)[0] preds=[pred_nb,pred_lr,pred_rf,pred_svm,pred_knn] spam_counts=preds.count(1) if spam_counts>=3: return 'Spam' return 'Non-Spam'
def get_probabilities(self,vector):
prob_nb=self.model_nb.predict_proba(vector)[0]*100 prob_lr = self.model_lr.predict_proba(vector)[0] * 100 prob_rf = self.model_rf.predict_proba(vector)[0] * 100 prob_knn = self.model_knn.predict_proba(vector)[0] * 100 prob_svm = self.model_svm.predict_proba(vector)[0] * 100 return [prob_nb,prob_lr,prob_rf,prob_knn,prob_svm]
def get_vector(self,text):
return self.vectorizer.transform([text])
3. Module – User interface
import time from ML import model import streamlit as st from DP import *
import matplotlib.pyplot as plt import seaborn as sns inputs=[0,1] @st.cache() def create_model(): mode=model() return mode
col1,col2,col3,col4,col5=st.columns(5) with col3:
st.title("Spade")
st.write('welcome to Spade...')
st.write('A Spam Detection algorithm based on Machine Learning and Natural Language Processing')
text=st.text_area('please provide email/text you wish to classify',height=400,placeholder='type/paste more than 50 characters here')
file=st.file_uploader("please upload file with your text.. (only
.txt format supported")
if len(text)>20: inputs[0]=1
if file is None: inputs[1]=0
if inputs.count(1)>1:
st.error('multiple inputs given please select only one
option') else:
if inputs[0]==1: e=text given_email = e
if inputs[1]==1:
bytes_data = file.getvalue()
given_email = bytes_data
predictions=[] probs=[] col1,col2,col3,col4,col5=st.columns(5) with col3:
clean_button = st.button('Detect')
st.caption("In case of a warning it's probably related to caching of your browser")
st.caption("please hit the detect button again....")
if clean_button:
if inputs.count(0)>1:
st.error('No input given please try after giving the
input') else: with st.spinner('Please wait while the model is
running....'): mode = create_model()
given_email,n=clean(given_email) vector = mode.get_vector(given_email) predictions.append(mode.get_prediction(vector)) probs.append(mode.get_probabilities(vector)) col1, col2, col3 = st.columns(3) with col2:
st.header(f"{predictions[0]}") probs_pos = [i[1] for i in probs[0]] probs_neg = [i[0] for i in probs[0]] if predictions[0] == 'Spam':
# st.caption(str(probs_pos)) plot_values = probs_pos
else:
# st.caption(str(probs_neg)) plot_values = probs_neg
plot_values=[int(i) for i in plot_values] st.header(f'These are the results obtained from the
models') col1, col2 = st.columns([2, 3]) with col1:
st.subheader('predicted Accuracies of models')
with st.expander('Technical Details'): st.write('Model-1 : Naive Bayes') st.write('Model-2 : Random Forest') st.write('Model-3 : Logistic Regression') st.write('Model-4 : K-Nearest Neighbors') st.write('Model-5 : Support Vector Machines')
with col2:
st.write('Model-1', plot_values[0]) bar1 = st.progress(0) for i in range(plot_values[0]):
time.sleep(0.01) bar1.progress(i)
st.write('Model-2', plot_values[1]) bar2 = st.progress(0) for i in range(plot_values[1]):
time.sleep(0.01) bar2.progress(i)
st.write('Model-3', plot_values[2]) bar3 = st.progress(0) for i in range(plot_values[2]):
time.sleep(0.01) bar3.progress(i)
st.write('Model-4', plot_values[3]) bar4 = st.progress(0) for i in range(plot_values[3]):
time.sleep(0.01) bar4.progress(i)
st.write('Model-5', plot_values[4]) bar5 = st.progress(0) for i in range(plot_values[4]):
time.sleep(0.01) bar5.progress(i)
st.header('These are some insights from the given
text.') entities=ents(text) col1,col2=st.columns([2,3]) with col1:
st.subheader('These are the named entities extracted
from the text') st.write('please expand each category to view the
entities') st.write('a small description has been included with
entities for user understanding') with col2:
if entities=='no':
st.subheader('No Named Entities found.')
else:
renames = {'CARDINAL': 'Numbers', 'TIME':
'Time', 'ORG': 'Companies/Organizations', 'GPE': 'Locations',
'PERSON': 'People', 'MONEY': 'Money',
'FAC': 'Factories'} for i in renames.keys():
with st.expander(renames[i]): st.caption(spacy.explain(i)) values = list(set(entities[i])) strin = ',  '.join(values) st.write(strin)
